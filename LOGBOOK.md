> For the sake of academic transparency, this logbook is created on Sun Oct 19 2025, which is way past the beginning date of this project. I will not backdate entries on Week 1 - 2 to misrepresent the actual timeline of development.

## Week 3: Oct 16 - Oct 22, 2025

### Goals

- Make a simple kueue job submission controller that works end-to-end.
- It should relay command from the CLI to kueue via the FastAPI controller, then to the kueue workload in the cluster.

### Considerations

- I thought about making a client-side cluster, removing the kueue and kubernetes entirely. The idea is to not make a job scheduler but only a coordinator that checks if jobs are running, and the user just uses it as a "spreadsheet" to coordinate this with their peers.
  - However, I decided against this because 
    - It would be reinventing the wheel
    - This makes actual distributed computing impossible.
    - Having to suspend someone else's job is not feasible in a client-side only model.
- We will stick to the motto of "let's use great technologies to build accessible applications", rather than "let's use accessible technologies to build great applications".
- Renamed to Qwex
  
### Activities

- Add RBAC manifests for the controller to manage kueue workloads.
  > Since this RBAC manifests only live in side the 

## What is Qwex?

Queued Workspace-Aware EXecutor (Qwex) is a modular run-this-on-another-machine system that consists of:

1. Backend-agnostic Zero-config Command Relay: Backend-agnostic is quite a scam because you need to setup an SSH server first. The idea is that once you have an SSH server, you can run `qwex batch python main.py` and it will run that job, and streams the output back to you. Wait, but do I need `scp`? No. `rsync`? No. `rsshfs`? No. Qwex will handle file transfer for you, so you don't need to worry about it.
2. Kueue-based Job Scheduling: While you can use Slurm as a backend, we propose another architectural design using [Kueue](https://kueue.sigs.k8s.io/). Kueue is a Kubernetes-native job scheduler that allows fine-grained control over job scheduling and resource allocation. Qwex leverages Kueue to manage job queues across multiple nodes, ensuring efficient resource utilization and job prioritization.

So can you just use `kubectl`? Yes.
Can you use `qwex` without Kueue? Yes.
Can you use `qwex`'s SDK to access your jobs programmatically? Yes.
It's modular and flexible.

## Week 6: Nov 6 - Nov 12, 2025

### Reevaluate Project Direction

- I'm using Kubernetes.
- It's easier to set up (trust me)
- It scales better.
- Easier to deploy.

Architecture: not yet finalized

## Week 8: Nov 20, 2025

### Runner Architecture Finalized

After extensive design discussions, we've finalized the architecture for Qwex's execution layer. The goal is to make Qwex a **zero-init, filesystem-level capture tool** that works with any code and any tracking tool.

#### Core Concepts

**Job vs Run**
- **Job**: Template/definition of work (the recipe)
  - Defines: command, environment, dependencies, configuration
  - Reusable and versioned
  - Example: "train model X with config Y"
- **Run**: Specific execution instance of a Job (one baking session)
  - Each run has unique ID, lifecycle, logs, outputs, metrics
  - Multiple runs can come from same job (retries, experiments, different params)
  - Tracks: status, timestamps, exit code, artifacts

**Runner vs Run Relationship**
- **Run**: Pure data model (state object)
  - Contains: ID, JobID, Status, CreatedAt, StartedAt, FinishedAt, ExitCode, Error, Metadata
  - Immutable/append-only
  - Persisted to filesystem or DB
- **Runner**: Execution engine (behavior/logic)
  - Implements: `Submit(ctx, jobSpec) (*Run, error)`, `GetRun(ctx, runID) (*Run, error)`, `Cancel(ctx, runID) error`
  - Stateless or minimal state (just config like API URLs, credentials, directories)
  - Two implementations:
    - **LocalRunner**: executes jobs on current machine
    - **QwexCloudRunner**: submits jobs to Qwex Cloud API (cloud execution uses LocalRunner internally)

#### Run Lifecycle States

Simplified state machine for MVP:
```
PENDING (submitted + queued + initializing)
  â†“
RUNNING (actively executing)
  â†“
SUCCEEDED | FAILED | CANCELLED
```

Future expansion:
```
PENDING â†’ INITIALIZING â†’ RUNNING â†’ FINALIZING â†’ SUCCEEDED/FAILED/CANCELLED
```

#### Directory Structure

**Nested structure** (matches wandb/mlflow patterns):
```
project/
  train.py
  out/                    # conventional output dir (user writes here)
    model.pth
    metrics.json
  .qwex/
    runs/
      abc123/
          metrics.json
        artifacts.json    # auto-generated manifest
- Single source of truth: everything for run X in one place
- Easy cleanup: `rm -rf .qwex/runs/abc123/`
#### Artifact Auto-Capture

**Zero-init approach**: Users just write to `out/` (or configured dir), qwex auto-captures.

**Default convention:**
```python
# User code - no qwex-specific logic needed
import torch
torch.save(model.state_dict(), "out/model.pth")
```

**Configuration** (`.qwex/config.yaml`):
```yaml
artifacts:
  watch_directories:
    - out              # default
    # - checkpoints    # user can add more
    # - models
  ignore_patterns:
    - "*.tmp"
    - "*.log"
    - "__pycache__"
    - ".git"
  max_file_size_mb: 1000
```

**Implementation**: Post-run directory scan (not real-time fsnotify)
- After command exits, scan configured directories
- Copy files to `.qwex/runs/<runId>/files/`
- Generate `artifacts.json` manifest with checksums

**CLI override:**
```bash
qwex run python train.py --local --watch outputs,models
```

#### Coexistence with Tracking Tools

**Design principle**: Qwex is **execution layer**, not **tracking layer**

**What qwex stores:**
- Execution metadata: status, command, exit code, timestamps
- Console output: stdout/stderr
- Artifacts: files from watched directories

**What qwex does NOT store:**
- Metrics/plots (leave to wandb/mlflow)
- Hyperparameters (except as command args)
- Training checkpoints (unless in watched dir)

**Example with wandb:**
```python
import wandb
import torch

wandb.init(project="my-project")

# Train model
# ...

# Both qwex and wandb capture from out/
torch.save(model.state_dict(), "out/model.pth")
wandb.save("out/model.pth")
```

**Result:**
- Qwex captures: `out/model.pth` â†’ `.qwex/runs/abc123/files/`
- Wandb captures: `out/model.pth` â†’ `wandb/run-xyz/files/`
- No conflicts â€” orthogonal systems

**Environment variables (optional):**
```bash
QWEX_RUN_ID=abc123
QWEX_RUN_DIR=/path/to/.qwex/runs/abc123
```

User code can optionally use these to link tracking runs:
```python
import wandb
import os

wandb.init(
    project="my-project",
    tags=[f"qwex:{os.getenv('QWEX_RUN_ID')}"]
)
```

#### User Experience

    - out
    - models
```

**Code (language-agnostic):**
```python

**Run:**
ðŸ“¦ Captured 1 artifact (50 MB)
```

**View artifacts:**
  out/model.pth (50.0 MB)


#### Key Benefits
âœ… **Tool-agnostic**: coexists with wandb/mlflow/tensorboard  
âœ… **Conventional**: `out/` is standard (like `node_modules/`, `build/`)  

1. Update `pkg/qsdk/runner/interface.go` with Job/Run separation and lifecycle states

## Week 9: Nov 26, 2025
#### Design Philosophy

- **Open protocol, flexible implementation**: Protocol is OSS, servers implement it however they want
- **No lock-in**: Your code doesn't know qwex exists. Remove `qwex run`, scripts still work
- **Coexists with W&B/MLflow**: qwex handles execution, they handle tracking

#### Protocol Manifest

Every qwex-compatible server exposes `/.well-known/qwex.json`:

```json
{
  "qwex": "1.0",
  
  "server": {
    "name": "string",
    "version": "string",
    "docs": "string (optional)"
  },
  
  "api": {
    "versions": ["v1"],
    "base": "/api"
  },
  
  "auth": {
    "type": "oauth2 | api-key | local | none",
    "spec": { }
  },
  
  "features": {
    "source": { },
    "artifacts": { },
    "runs": { }
  }
}
```

#### Auth Types

```json
// api-key (v1 focus)
"auth": {
  "type": "api-key",
  "spec": {
    "header": "X-API-Key"
  }
}

// local (for qwex-local daemon)
"auth": {
  "type": "local",
  "spec": {
    "keyFile": "~/.qwex/local-api-key"
  }
}

// oauth2 (future)
"auth": {
  "type": "oauth2",
  "spec": {
    "provider": "github",
    "authUrl": "/auth/login",
    "tokenUrl": "/auth/token"
  }
}
```

#### Features (Behavior Flags)

Features don't change API shape, they change behavior:

```json
"features": {
  "source": {
    "transfer": "presigned-upload | mount",
    "format": "git-bundle | tar | worktree",
    "maxSize": 104857600,
    "requireClean": true
  },
  
  "artifacts": {
    "enabled": true,
    "autoUpload": true,
    "patterns": ["*.pt", "*.onnx", "outputs/**"],
    "maxSize": 1073741824
  },
  
  "runs": {
    "wait": true,
    "cancel": true,
    "logs": true,
    "backends": ["local", "docker", "kueue"]
  }
}
```

#### Fixed API Routes (v1)

| Method   | Route                                      | Description                         |
| -------- | ------------------------------------------ | ----------------------------------- |
| `POST`   | `{base}/v1/source/upload-url`              | Get presigned URL for source upload |
| `POST`   | `{base}/v1/runs`                           | Submit run                          |
| `GET`    | `{base}/v1/runs`                           | List runs                           |
| `GET`    | `{base}/v1/runs/{id}`                      | Get run                             |
| `DELETE` | `{base}/v1/runs/{id}`                      | Cancel run                          |
| `GET`    | `{base}/v1/runs/{id}/logs`                 | Get logs                            |
| `GET`    | `{base}/v1/runs/{id}/wait`                 | Wait for completion                 |
| `GET`    | `{base}/v1/runs/{id}/artifacts`            | List artifacts                      |
| `GET`    | `{base}/v1/runs/{id}/artifacts/{name}/url` | Get artifact download URL           |
| `POST`   | `{base}/v1/runs/{id}/artifacts/upload-url` | Get artifact upload URL             |

#### Source Code Transfer

**Problem**: How does remote execution get your code?

**Solution**: Git bundle approach (strict mode)
- Must commit before run (enforces reproducibility)
- Client creates git bundle of committed code
- Bundle uploaded to S3 (for remote) or worktree created (for local)
- Every run tied to exact git SHA

**Local runs**: Git worktree (instant, no network)
```bash
git worktree add /tmp/qwex-runs/<run-id> <sha> --detach
# Container mounts this directory
# After run: git worktree remove ...
```

**Remote runs**: Git bundle + S3
```bash
git bundle create /tmp/repo.bundle --all
# Upload to s3://qwex/source/<run-id>/repo.bundle
# Container clones from bundle
```

**Benefits**:
- No GitHub token needed (bundle contains everything)
- Perfect reproducibility (exact SHA)
- `qwex checkout <run-id>` can recreate exact state
- Your working dir stays untouched while run executes

#### Example Manifests

**qwexcloud (production)**:
```json
{
  "qwex": "1.0",
  "server": { "name": "qwexcloud", "version": "1.0.0" },
  "api": { "versions": ["v1"], "base": "/api" },
  "auth": { "type": "api-key", "spec": { "header": "X-API-Key" } },
  "features": {
    "source": {
      "transfer": "presigned-upload",
      "format": "git-bundle",
      "maxSize": 104857600,
      "requireClean": true
    },
    "artifacts": {
      "enabled": true,
      "autoUpload": true,
      "patterns": ["*.pt", "outputs/**"]
    },
    "runs": {
      "wait": true,
      "cancel": true,
      "logs": true,
      "backends": ["kueue"]
    }
  }
}
```

**qwex-local (development)**:
```json
{
  "qwex": "1.0",
  "server": { "name": "qwex-local", "version": "1.0.0" },
  "api": { "versions": ["v1"], "base": "/api" },
  "auth": { "type": "local", "spec": { "keyFile": "~/.qwex/local-api-key" } },
  "features": {
    "source": {
      "transfer": "mount",
      "format": "worktree",
      "requireClean": true
    },
    "artifacts": { "enabled": false },
    "runs": {
      "wait": true,
      "cancel": true,
      "logs": true,
      "backends": ["local", "docker"]
    }
  }
}
```

#### Request/Response Types (v1)

```yaml
# POST /v1/source/upload-url
SourceUploadUrlRequest:
  sha: string
  size: integer

SourceUploadUrlResponse:
  url: string
  key: string
  expires: integer

# POST /v1/runs
SubmitRunRequest:
  name: string (optional)
  command: string
  args: string[]
  env: map[string]string
  source:
    sha: string
    key: string
  backend: string (optional)

RunResponse:
  id: string
  name: string
  status: "pending" | "running" | "succeeded" | "failed" | "cancelled"
  command: string
  args: string[]
  createdAt: string (RFC3339)
  startedAt: string (optional)
  finishedAt: string (optional)
  exitCode: integer (optional)
  backend: string
  source:
    sha: string
  artifacts: Artifact[]

LogsResponse:
  stdout: string
  stderr: string

ArtifactUrlResponse:
  url: string
  expires: integer
```

#### Implementation Plan

**Phase 1: Protocol Foundation**
1. Create `docs/protocol/qwp-v1.md` â€” Full spec
2. Create `pkg/qwp/` â€” Go types for manifest + requests/responses
3. Create `api/qwp/openapi.yaml` â€” OpenAPI spec for v1 routes

**Phase 2: qwex-local Server**
4. Create local server implementing QWP v1
   - Auth: api-key (simple header check)
   - Source: mount (git worktree)
   - Runs: local + docker backends
   - Artifacts: disabled initially

**Phase 3: qwexctl Refactor**
5. Create `pkg/qwp/client.go` â€” Protocol client
6. Refactor qwexctl to use qwp client
   - `qwexctl init <server>` â€” Register server
   - `qwexctl run` â€” Use protocol

**Phase 4: qwexcloud Alignment**
7. Update qwexcloud to implement QWP v1
   - Add `/.well-known/qwex.json`
   - Align routes to spec
   - Add source upload endpoint

## Week 9: Nov 29, 2025

### Go Code Removal

In preparation for the new Python-based daemon and client architecture, we have removed the legacy Go codebase. This allows us to focus on a single language stack and iterate faster on the new daemon design.

**Safety Measures:**
- A backup branch was created before deletion: `wip/go-backup-20251129T115607Z`
- The deletion commit on `main` is: `45323bc`

**Removed Components:**
- `apps/qwexcloud` (Go server)
- `apps/qwexctl` (Go CLI)
- `pkg/*` (Go packages: k8s, qapi, qrunner, etc.)
- `go.mod`, `go.sum`

**Next Steps:**
- Implement the Python daemon with UDS/HTTP support.
- Refactor `qwexctl` (Python) to communicate with the daemon.
- Implement the plugin system for runners.

---

## Week 10: Dec 2, 2025

### Execution Model Finalized: Git Worktree Isolation

After discovering design flaws in the current implementation, we're documenting the correct execution model.

#### Core Invariant

**Every run executes in a git worktree, not the working directory.**

This ensures:
1. **Reproducibility**: Run tied to exact commit SHA
2. **Isolation**: Concurrent runs don't interfere
3. **Clean state**: No uncommitted changes affect execution

#### QWEX_HOME Structure

```
$QWEX_HOME/                    # ~/.qwex by default, or workspace/.qwex
  repos/                       # bare git repos (for remote storage)
    <workspace-name>.git/
  spaces/                      # ephemeral worktrees (deleted after run)
    <run-id>/
  runs/                        # persistent run metadata + logs
    <run-id>/
      run.json                 # MUST have commit field
      statuses.jsonl
      stdout.log
```

#### Run Lifecycle

```
1. User runs: qwex run python train.py

2. Pre-flight checks:
   - Is this a git repo? (required)
   - Get current commit SHA (required, stored in run.json)

3. Create worktree:
   - git worktree add --detach $QWEX_HOME/spaces/<run-id> <commit>

4. Execute in worktree:
   - cd $QWEX_HOME/spaces/<run-id>
   - python train.py > $QWEX_HOME/runs/<run-id>/stdout.log 2>&1

5. Cleanup:
   - git worktree remove $QWEX_HOME/spaces/<run-id>
   - Run metadata persists in $QWEX_HOME/runs/<run-id>/
```

#### run.json Schema (Required Fields)

```json
{
  "id": "uuid7",
  "commit": "abc123...",        // REQUIRED - for reproducibility
  "workspace_name": "my-project",
  "command": "python",
  "args": ["train.py"],
  "status": "succeeded",
  "created_at": "...",
  "started_at": "...",
  "finished_at": "..."
}
```

#### Storage Backends

Storage handles syncing code to remote execution environments.

**git-direct**: Push to bare repo via SSH
- Used with SSH layer
- Remote creates worktree from pushed commit
- `base_path` should come from layer's `qwex_home`, not hardcoded

**mount**: Direct filesystem mount
- Used with local/docker execution
- No transfer needed, worktree created locally

#### Layer + Storage Relationship

```yaml
# qwex.yaml
layers:
  ssh:
    type: ssh
    host: csc
    qwex_home: ~/.qwex        # remote QWEX_HOME

storage:
  code:
    type: git-direct
    layer: ssh                # inherits ssh_host and qwex_home from layer

runners:
  remote:
    layers: [ssh]
    storage:
      source: code            # uses git-direct storage
```

Storage config should reference layer to avoid duplication:
- `ssh_host` comes from layer
- `base_path` = `{layer.qwex_home}/repos`

#### Registry Decorators

Simplified decorator names:
- `@layer` instead of `@register_layer`
- `@storage` instead of `@register_storage`

#### Checkout a Run

To reproduce a run:
```bash
qwex checkout <run-id>
# reads run.json, gets commit, creates worktree
git worktree add ./run-<run-id> <commit>
```

#### Key Fixes Needed

1. **Run must require commit**: No run without git commit
2. **Storage depends on layer**: `git-direct` gets `ssh_host` and `base_path` from SSH layer
3. **Worktree is mandatory**: All execution happens in worktrees
4. **Shorter decorators**: `@layer`, `@storage`

---


## Week 10: Dec 3, 2025

### Template-Based Architecture: "shadcn for shell scripts"

After reflecting on the current Python-heavy implementation, we're exploring a radical simplification: **qwex as a template compiler**.

#### Core Insight

> "qwex is just a compiler that turns YAML + templates into a single executable command"

Instead of hiding layers behind Python abstractions, expose them as **user-owned shell script templates** that users can inspect, copy, and modify â€” like [shadcn/ui](https://ui.shadcn.com/) for shell scripts.

#### Architecture: Rigid Core + Flexible Templates

| **Core (rigid, built-in)** | **Templates (user-owned, `.qwex/templates/`)** |
|---|---|
| Run lifecycle management | SSH connection/execution |
| `run.json` generation & tracking | Docker container execution |
| Run ID generation (UUID) | Slurm job submission |
| Logging infrastructure | Singularity/Apptainer |
| `qwex list` / `qwex logs` / `qwex cancel` | Kubernetes Job templates |
| Artifact tracking (inputs/outputs) | Git worktree setup |
| Process detachment / background runs | Storage mount commands |
| Exit code capture & status | Custom wrappers (conda, nix, etc.) |
| Backend storage for run metadata | |
| Template rendering engine (Jinja) | |

#### Directory Structure

```
.qwex/
  templates/           # user-owned, inspectable shell templates
    ssh.sh.j2
    docker.sh.j2
    slurm.sh.j2
    singularity.sh.j2
    worktree.sh.j2
  targets/             # gitignored - secrets/host configs
    hpc.yaml
```

#### Template Contract

Each template:
1. Receives context variables (run ID, workspace, layer config, etc.)
2. Wraps `{{ inner }}` or `{{ command }}` 
3. Outputs valid shell

**Example `ssh.sh.j2`:**
```bash
#!/bin/bash
ssh {{ layers.ssh.user }}@{{ layers.ssh.host }} bash -c '
{{ inner | indent(2) }}
'
```

**Composition (ssh â†’ docker):**
```bash
# Rendered for: qwex run -r hpc echo hello
ssh qtn@csc bash -c '
  docker run --rm ghcr.io/astral-sh/uv:latest bash -c '
    echo hello
  '
'
```

#### Comparison with Similar Tools

| | GitHub Workflows | Taskfile | qwex |
|---|---|---|---|
| **Focus** | CI/CD on GitHub runners | Local task runner | Remote execution anywhere |
| **Templates** | Built-in actions | None | User-owned `.j2` files |
| **Registry** | GitHub Marketplace | None | `qwex add ssh` (shadcn-style) |
| **Composition** | Steps in sequence | Task dependencies | Nested shell wrappers |

#### Design Questions (Open)

1. **Language**: Go (single binary, `text/template` stdlib) vs Python (already working, faster iteration)?
   - **Decision**: Validate in Python first, port to Go if distribution matters.

2. **GitHub Workflow syntax**: Worth inheriting for familiarity, or too heavy?
   - **Decision**: Keep `qwex run` simple. Workflows syntax is optional power feature.

3. **Overlay/patching**: Kustomize-like JSON/YAML patching?
   - **Decision**: Defer. Scope creep. Templates are enough for MVP.

#### Next Steps

1. Prove concept: Make `qwex run -r ssh echo hello` use a `.qwex/templates/ssh.sh.j2` template instead of Python code.
2. Add second layer (docker) and prove composition works.
3. Only then consider Go rewrite, registry, etc.


---

## Dec 3, 2025: Global Qwex Home Architecture

### File-Based Metadata Storage Revolution

**Problem**: JSON manipulation in bash is too limited for complex operations. Need a simpler way to store and update run metadata.

**Solution**: Replace JSON with file-based key-value storage where each metadata field is stored in its own file:

```
$HOME/.qwex/$WORKSPACE_NAME/runs/$RUN_ID/
â”œâ”€â”€ meta/
â”‚   â”œâ”€â”€ id
â”‚   â”œâ”€â”€ command_line  
â”‚   â”œâ”€â”€ status
â”‚   â”œâ”€â”€ created_at
â”‚   â”œâ”€â”€ started_at
â”‚   â”œâ”€â”€ finished_at (optional)
â”‚   â””â”€â”€ exit_code (optional)
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ stdout.log
â”‚   â””â”€â”€ stderr.log
```

**Benefits**:
- Bash can easily read/write individual files with `>` and `cat`
- No JSON parsing complexity
- Simple directory structure for run discovery
- Easy to extend with new metadata fields
- Atomic updates (each file is independent)

**Implementation**:
- Workspace name extracted from `qwex.yaml` `name:` field
- Global qwex home at `$HOME/.qwex/$WORKSPACE_NAME/runs/`
- File-based metadata creation and status updates
- Preserves working directory and environment

**Test Results**: Successfully tested from workspace root and deep subdirectories. Run lifecycle (create â†’ running â†’ succeeded/failed) works correctly with proper timestamp tracking.

---

## Dec 5, 2025: Asking questions


1. What core problem are you actually trying to solve? Beyond the surface-level features (running jobs on SSH/SLURM/Singularity, etc.), what's the fundamental pain point or inefficiency in distributed computing/ML workflows that motivated qwex? Why isn't this just a wrapper around existing tools like Kubernetes, Ray, or even plain SSH scripts? What evidence do you have that this problem is widespread enough to warrant a new tool?

> Sure. Due to the emergence of AI, machine learning models have become smaller and smarter. The foundational model became a commodity. The code, the sdk, the agentic pipeline, everything has become open-source. the last thing is that the infrastructure does not yet have a standard protocol. the research codes (public not so much, but especially private e.g. in github orgs) are notoriously difficult to reproduce. here's the data. here's the source. run it on any gpu or container or whatever you want! and that has wasted a lot of researcher's time. Moreover if the machine is shared, now you get stuck in that env and wasted idle GPU hours when it could've been used by someone else. It is a wrapper around existing tools, but it's not a wrapper of one existing tool. It's a wrapper of any tool you want to integrate. It's not even "evidence". This is a well-known, widespread problem as confirmed by my interview. Everybody has their own ways of doing things.

2. Why this modular, layered architecture (with qwml, qwp, qwexcli) instead of a monolithic or simpler design? Looking at your workspace structure, you've split things into engines, layers, commands, and models. What drove this decomposition? Is it truly necessary for extensibility, or is it premature abstraction that adds complexity without proven benefits? How do you justify the overhead of maintaining separate packages (pyproject.toml files everywhere) versus a single, cohesive codebase?

> This solves the N x M problems. It's like well you could have just pack everything into one html file. Why make react? now it makes swapping things in and out much easier. Modules should be first-class support for a programming language. (looking at you C++). Sure it could be a premature abstraction, but saying that it's a premature abstraction means there is a proof that this is a premature abstraction, then that research proving such a thing could be a great contribution. Also there is no pyproject.toml. it's just ssh.sh.j2. can you can just call qwex run. it's cohesive. I'm managing multiple files because im stupid ok?

3. What are the key trade-offs in your runner system (SSH, Docker, Singularity, etc.), and why prioritize these over cloud-native alternatives? Why not build on top of something like AWS Batch, Google Cloud Run, or even Kueue directly (since I see kueue configs in your infra)? Are you assuming on-prem or hybrid environments are the primary use case? What data or benchmarks show that your custom runners outperform or simplify compared to off-the-shelf solutions?

> Because I need a baseline to say that: as you can see cloud native integrations are much easier to do. And see the trade-off by the complexity itself. On K8S you might know you have to setup PVC? Ingress? you could have just place an agree-upon cache folder in user dir! On SSH you might see wtf k8s has kubectl secrets and it just does things for you automatically??? i had to envelope things and encrypt it. Then you can see both tradeoffs very clearly instead of doing undocumented works. Also it's not a custom runner is a runner compiler!

4. How do you handle failure modes and edge cases in job execution? From the terminal output, I see errors like "No such file or directory" and panics in Rust (uv-related). Why not design for robustness from the startâ€”e.g., better error handling, retries, or state persistence? Is this a deliberate minimalism, or an oversight that could undermine reliability in production?

> so there are cases where it's user error. cases where you pulled a footgun on yourself. it's not minimal because it's like a port where you can plug everything in.

5. What makes qwex uniquely valuable compared to competitors like MLflow, Prefect, or Dask? Why would someone choose qwex over these established tools? What novel insights or innovations justify the effort? If it's about simplicity or integration with specific stacks (e.g., Singularity on SLURM), quantify the benefitsâ€”e.g., performance metrics, user adoption, or reduced setup time.

> You can use MLflow Prefect or Dask under qwex so that's not a good question honestly. qwex is a shell compiler. so you can just make dask.sh.j2 it's just that now the under-layer has a good starting point where you can compose things. This wasn't achievable with Makefile because there's no Makefile distro or anything like that.

6. Finally, what's your thesis's overarching narrative? How does qwex fit into the broader story of distributed systems or ML engineering? Are you claiming it's a paradigm shift, or just an incremental improvement? What risks are you downplaying, and how will you defend against criticisms that this is "yet another orchestration tool" with unclear differentiation?

> It's a paradigm shift. because now it fills the gap between the last layer of knowledge sharing in ML space: executing the code. Everything else is shared but not this specific part.

## Reflection Log: Why This Design, Not Another Way

### Overview
This reflection log synthesizes the design rationale for Qwex (Queued Workspace-Aware EXecutor) based on critical questioning and self-examination. As a thesis foundation, it addresses why certain architectural choices were made, trade-offs considered, and alternatives rejected. The analysis draws from interviews (N=15 with ML engineers, students, and professors), project evolution, and ongoing development challenges. Critically, Qwex positions itself as a "paradigm shift" in ML infrastructure by filling the execution gap in knowledge sharing, but this claim remains unproven without empirical data.

### Core Problem and Motivation
**Why this problem?** The emergence of AI/ML commoditization has made models, code, SDKs, and pipelines open-source, but infrastructure lacks standardized protocols. Research code (especially private) is notoriously hard to reproduce: "here's the data, run it on any GPU/container!" wastes researchers' time and GPU resources. Interviews revealed diverse pain pointsâ€”burning money on idle VMs, ad-hoc spreadsheets for reservations, overnight runs on personal PCsâ€”highlighting socio-economic barriers to standardization adoption. Qwex isn't a wrapper for one tool but a "standard-gatherer" for any integrable tool, allowing users to compose solutions without imposing new standards.

**Why not alternatives?** Pushing for universal protocols (e.g., a job spec standard) was rejected due to adoption inertia. Existing tools (K8s, Ray) solve parts but not composition across heterogeneous environments. Qwex acts as "protective gear" over "hot lava," enabling reproducibility without waiting for ecosystem consensus.

**Critical doubt:** Without quantitative metrics (e.g., time waste quantification), this is anecdotal. The thesis risks being dismissed as solving a "first-world problem" in academia.

### Architecture and Modularity
**Why modular (qwml, qwp, qwexcli)?** To solve "N x M problems"â€”combinatorial explosions from multiple runtimes (N) and backends (M). Analogous to React's component modularity vs. monolithic HTML: swapping layers (e.g., engines, layers, commands) is easier. Modules are "first-class" like in programming languages (unlike C++'s lack thereof). Fragmentation (multiple pyproject.toml files) stems from iterative pivots, not design intentâ€”admitted as "stupidity" in managing files.

**Why not monolithic or DSL?** Monolithic would limit extensibility. A DSL was avoided to prevent "yet another standard," aligning with the anti-standard ethos. Instead, shell templates (e.g., ssh.sh.j2) provide cohesion: "just call qwex run."

**Critical doubt:** Modularity may be premature without proof of N x M benefits. Fragmentation indicates instability; if unaddressed, it undermines maintainability. Evidence needed: user studies showing modularity reduces complexity.

### Runner System and Trade-Offs
**Why runners as a "compiler," not custom?** Provides a baseline for comparing trade-offs: cloud-native ease vs. on-prem simplicity (e.g., K8s PVCs/ingress vs. user-dir cache). Shows complexities clearly (e.g., SSH envelope encryption vs. K8s secrets). Not custom runners but a compiler generating execution scripts.

**Why prioritize SSH/SLURM/Singularity over cloud-only?** Interviews showed hybrid/on-prem needs. Cloud tools (AWS Batch) are easier but obscure trade-offs; Qwex makes them explicit.

**Critical doubt:** No benchmarks yet (project 50% done). Claims of "much easier" integrations are speculative. Evaluation via Olsen's HCI criteria is promising but unappliedâ€”how will it validate effectiveness?

### Failure Modes and Robustness
**Why minimal error handling?** Assumes user errors ("footguns") are inevitable; Qwex is a "port" for plugging in, not a safety net. Only bugs in compilation are "real" errors. Deferred for later.

**Why not prioritize robustness?** Focus on MVP execution over perfection.

**Critical doubt:** Logs show file-not-found and Rust panicsâ€”unacceptable for production. Skipping this weakens the thesis; robustness is key for reproducibility claims.

### Uniqueness and Competitors
**Why composable over wrappers?** Users have different requirements (e.g., Ray migration pains); keeping runtime outside code avoids lock-in. Shell compiler enables composition (e.g., dask.sh.j2) beyond Makefiles' limitations. Integrates MLflow/Prefect/Dask without replacing them.

**Why not use existing tools directly?** They don't compose across backends or isolate runtimes.

**Critical doubt:** If Makefiles can do similar with plugins, what's the unique value? Examples needed to prove superiority.

### Overarching Narrative and Risks
**Why a "paradigm shift"?** Fills the "last layer" of ML sharing: executing code. Weights & Biases tracks experiments assuming code runs; GitHub Actions handles CI/CD. Qwex enables reproducible execution across infrastructures.

**Risks downplayed:** Abandonment (biggest fear)â€”mitigated by pivoting (e.g., from prototype 1). No Plan B beyond "think of a way to pivot." Project incompleteness (50% done, no data) risks thesis failure.

**Critical doubt:** "Paradigm shift" is bold without adoption evidence. If abandoned, the thesis becomes a cautionary tale of overambition. Pivot to studying socio-economic factors instead?

### Lessons and Thesis Implications
This design reflects pragmatism over perfection: build what works for users, avoid standards wars. But critical gaps (data, robustness, stability) must be addressed. For the thesis, emphasize iterative design, user-centered insights, and the "standard-gatherer" metaphor. Future work: complete evaluation, stabilize architecture, gather metrics. If Qwex succeeds, it validates the approach; if not, it highlights infrastructure standardization's challenges.


## Week 10: Dec 7, 2025

 - Added a minimal template scaffold and project scaffolding improvements for the CLI package `apps/qwexcli`:
  - `apps/qwexcli/qwexcli/templates/base.py`: simple template exposing `run(argv)` which executes a command.
  - `apps/qwexcli/qwexcli/lib/project.py`: scaffold now writes explicit `defaults` and `runners` into `.qwex/config.yaml` and creates `.qwex/.gitignore` to ignore internal artifacts.
  - `apps/qwexcli/qwexcli/lib/config.py`: removed `workspaces` from the schema; added `defaults` and `runners` fields (defaults.runner -> "base", runners.base.templates -> ["base"]).
  - Tests updated to assert the new defaults and the presence of `.qwex/.gitignore`.

---

## Week 11: Dec 8, 2025

### Crisis of Contribution

Today I hit a wall. The fundamental question: **what is the novel contribution?**

The current implementation boils down to: "you can put a runner in a file and call it to wrap your command." That's it. That's the whole thing. A tweet could explain it. A GitHub gist could contain it. So why is this a thesis?

#### The Problem with "Just a Wrapper"

The devil's advocate argument (which I made myself and now regret): this is just a command wrapper anyone could write. The counterargument I need to articulate clearly:

1. **It's not the wrapper itselfâ€”it's the protocol and composition model.**
   - Anyone can write `ssh user@host 'python train.py'`
   - But nobody has standardized how to compose SSH â†’ Docker â†’ SLURM â†’ Singularity in a way that's inspectable, reproducible, and user-owned.

2. **The "shadcn for shell scripts" insight is actually novel.**
   - User-owned templates (not hidden abstractions)
   - Composable layers (not monolithic tools)
   - Transparent execution (you can read the generated script)

3. **The gap being filled: executable reproducibility.**
   - W&B tracks metrics assuming your code runs
   - GitHub shares code assuming you can run it
   - Papers share "run this command" assuming you have the environment
   - Qwex bridges: "here's exactly how to run this, on any backend, reproducibly"

#### What I Actually Built This Week

- Added `qwex run -r <runner> <cmd> <args>` command to CLI
- Runner lookup: finds `.qwex/runners/<runner>.py` and executes it
- Runner contract: accepts `--command` and `--args` (JSON array)
- Created `noop.py` as reference implementation with `Runner` class, `make_runner_from_dict()` factory, and `if __name__ == "__main__"` entrypoint

#### The Contribution I Need to Articulate

**Not this:** "Hey, you can wrap commands with Python files!"

**But this:** "A protocol for composable, transparent, user-owned execution templates that enable reproducible ML runs across heterogeneous infrastructure without imposing new standards."

Key properties that make it novel:
- **Transparent**: Users can read and modify the generated execution scripts
- **Composable**: Layers stack (SSH wraps Docker wraps command)
- **User-owned**: Templates live in `.qwex/`, not hidden in a library
- **Backend-agnostic**: Same interface for local, SSH, SLURM, K8s
- **Zero lock-in**: Remove qwex, your code still runs

#### What's Missing for the Thesis

1. **Empirical validation**: Need to actually run user studies showing this approach reduces friction
2. **Comparison benchmarks**: Quantify setup time vs. alternatives (raw scripts, Makefiles, existing tools)
3. **Real-world case studies**: Document actual researchers using this for actual papers
4. **The NÃ—M proof**: Show that adding a new backend (N) or new runtime (M) is O(1) not O(NÃ—M)

#### Honest Self-Assessment

- Project is ~50-60% complete
- No empirical data yet
- Architecture has stabilized (finally)
- The "paradigm shift" claim is bold but unproven
- Risk: this becomes a cautionary tale of overambition

#### Next Steps

1. **Stabilize the runner interface** - finalize the contract (`run()`, `make_runner_from_dict()`, CLI args)
2. **Implement SSH runner** - prove composition works (local â†’ SSH â†’ command)
3. **Document the protocol** - write `docs/protocol/qwp-v1.md` properly
4. **Start evaluation** - design user study, prepare comparison benchmarks
5. **Stop second-guessing** - the contribution is real, I just need to prove it

#### Emotional State

Frustrated. Oscillating between "this is trivial" and "this is actually important." The curse of working on infrastructure: when it works, it's invisible. Nobody thanks the plumber until the pipes burst.

The thesis will either validate that transparent, composable execution templates are a meaningful contribution to ML infrastructureâ€”or it will document why this approach failed and what that teaches us about the space.

Either way, it's research.

## Week 11: Dec 9, 2025.

Yooooo. here's a quick dump:

so it must have 4 main stages: 
1. packing up and sending this to where the remote machine can retrieve
2. telling what the remote machine needs to do to get this job started
3. keeping the stdout/stderr observable by default.
4. retrieve the result back anytime.

most of the tools on the market assume you have 1. ready and just does 2. or blur the line between 1 and 2 so much you can't migrate. they also assume 3 is easy to do by default if you work with 2 long enouguh. and 4? "eh not my job" -- they said.

while the python cli can be powerful and use any buildtime dependencies you want.
the runtime capability is one of the hardest problem you have to think about. 
what is the least requirement to make it achieve the same robust execution on any machine you run?

we need to define what makes a run "robust" because i have been throwing this jargon left and right without defining what it means. robust in dictionary means strong and showing vigorous health. so what i mean is that it's fault-tolerant and "just works".
1. can start by itself non-interactively: have everything ready before the run.
2. show that it's alive: you know that it's moving. if you have tqdm in your python script it should in real-time show the epoch of your run for you to estimate how long it might have left.
3. recoverable when it got teared down: you can still recover the output after it's done. containers are ephemeral and especially k8s pods where they just "hehe you don't need this anymore? gbye~".

1 implies that source code, 3rd-party data from network storage, dependencies, must be in-place before run. 2  implies that runtime logs must be saved somewhere standard enough for every run to be the same. 3 implies that when the runs ended, someone who has no information of the run knows what and where to upload the artifacts of those runs. 

if you are reading until this point and thought hey that's not what i usually do, i'd rather... 
yes, that's what makes this a research and that's what makes this incredibly difficult. there is no industrial standard to this and everybody simply has a standard to themselves, write their own scripts that work for them, yet the most difficult part of machine learning research is: time to replicate an experiment. you have all the source codes and where to get the data but making this code run in the same environment, the same hardware conditions is not easy.

a good way to start is asking: here's a very simple codebase. it downloads a model from huggingface and generate text from prompt: "write me a python calculator". everything is prewritten so you just have to run it. on the otherside you have an HPC cluster with unknown spec for the environment but it uses slurm and RTX A2000 (maybe). go

you would say
```
ssh user@host

# oh yeah i need the code
exit
# scp repo.....

# .....

# where should i place it
scp repo user@host:~/workspaces/repo
already exist

# damn
scp repo user@host:~/workspaces/repo-2

uv sync # ?
uv not installed

# damn it
which python
/usr/bin/python

python -m pip install uv
Requirement satisfied!

uv sync

uv run python main.py
sorry it's a gated model. huggingface token pls

# damn it
uv pip install huggingface-cli

huggingface-cli login --token $TOKEN

uv run python main.py
CUDA out of memory pls find another machine

# restart all of this

uv run python main.py
def calculate(expression: str):
  # it is technically RCE so don't use this in prod
  return eval(expression) 
```

nice you made it!
now i don't want a calculator anymore. i want a haiku about cincinnati zoo in 2016. export it to txt. and save it in your local computer.

then you realized oh i shoulda use git.

now your workflow changes as you realized things. 

it's inefficient. im tired of that too.

so qwex is here to fix that. you write code. you add what you need. you run it as if you are sitting in a restaurant. line cook will work it out for you. it's not another vscode-server but sure it has a layer that works well on vscode-server. it's not another gitpod but sure it has a layer that works well with it.  it's not slurm it's not k8s. but it's the last IaC you need.

so what do we do? it is a launcher who will help package our job bundle to be run in another environment, nail a hole to make it observable, and eventually retrievable, while also respecting the job scheduler or minimizing the usage on the receiving end. mind you that we don't aim to save any cost through minimizing the usage because cost depends on a lot of factors and should be saved by other means. (analogy: you don't go on a diet just because to save your food cost) it's just a metric to help us build an efficient shared compute resource. if it does help you save cost then good for you.

ok. cool. now we have a place to sit, a target to solve. next is how we can solve it.

the problem of making a tool like this is distribution.

it can't be a webapp because browser is contained execution environment. it cannot ssh (it would be a huge workaround to make that work). it cannot work with lower level of your computer. ok. it cannot be desktop app because you would already have a code editor open and you have to switch the window back and forth. the only sensible thing is a cli because most code editor would have a terminal (or is already running in a terminal by itself lol). and you would already run python main.py in a terminal

first i have a range of languages i can choose, though it might be obvious that it should be python, but asking why not go, javascript, or rust, or any recreational interpreted languages out there. the main reason is that it can't be compiled language due to the need of development speed not runtime speed. so in the world of interpreted languages we have javascript, python, lua, ruby, and elixir, and more. but we can only choose python due to the fact the the code must be readable by machine learning engineer fellows, and the ability to just pip install our cli.
sure so it's a python cli.

because it makes sure a command is run on another machine. the user would either run `qwex shell` to make the play-field like when you do `source .venv/bin/activate` or `ssh` (change the context of the shell) or `qwex run <command>` to do the same thing for one-off command. (usually the latter is preferred). so it's a command wrapper / transformer.

what do we do when `python main.py` is received as an input? 
first we bundle this source code. we could tarball the entire thing and send everything along with the wire but that would be very inefficient for a larger codebase, especially when we do this a couple of times. it might also be difficult to separate what should be ignored. the industrial standard is using git. this also comes with perks that you can send only a few commits on every run  (or none!) making this highly efficient. so now the first dependency we need is git. 

there are millions way to send a git repository to another machine. 
1. git bundle pros: it should work in every case as long as the recipient can download that bundle cons: incremental update can be difficult. you need to create a new bundle every time.
2. git direct push pros: no download needed because it pushes to the target repository directly. cons: works only if you have ssh access
3. git push pros: just work. industrial standards cons: needs origin/git server. needs read credentials.

we should keep all of these in mind.

so packaging is a success.

next, submission. submission differs wildly across different machines and its scheduler backend. some requires access via ssh.... or not if you're already in it. 
it must be self-contained and should not have references to the files in your repository (for job description. of course it might reference it for runtime purposes like main.py but not `cat job.yaml`) the details on how it should be implemented will be discussed later.

---

yeah the obvious answer to this is template file with overrides. we will think about the template and override logic later but let's move on. i'd call submission a success.

next it's the lifecycle management. this is the part where it should be shared, backend-agnostically. could be splitted into: up, run, and down. up station would contain: pre-source and post-source. (will discuss about that in a minute) run station would contain: running the script (of course lol) and piping and teeing the logs to an appropriate location recoverable by down station. down station would contain: putting these artifacts from specified directory into a safe-space.

this is tightly linked to the scheduler backend because each scheduler implements it differently. for slurm you kinda have to make this multi-stage. for k8s you have init containers and prestop hook. for everything else you kinda have to make a trap by yourself.

let's come back to pre-source and post-source. 

pre-source is the part where you have environment-agnostic idempotent task like git clone, rclone or mounting nas to your scheduler's accessible environment. git worktree add. 
this is tightly linked to the packaging. you should know where to clone to. and have the secrets ready. you also could not reference anything inside your code. (because this is pre-source) so everything is run here. this should require almost no extra dependencies except you wanna do aws s3, rclone, or git. but yeah the design consideration is how to reduce the dependencies of this part.

post-source is the part where you can have more environment-specific tasks like uv sync.
yeah mostly just that. this is tightly linked to containerization option because environment-specific task would require extra packages.

down station should share the dependencies with pre-source. what it does is uploading metadata, logs, and artifacts to the predefined bucket. this is tightly linked to the retrieval stage.

retrieval is the easiest part if it has the correct file system format (path format) to the storage we predetermined. it just downloads them and could request for a specific dependency interatively.

the most difficult part here is lifecycle hooks. because it links to every stage. (will discuss the design later)

don't comment a lot like you did before. don't suggest new ideas. just ask questions if i missed anything that i promised i will talk about.

---

coming back to template and override logic that i forgot to talk about.

should this be a plug-and-play or a framework-like (like taskfile) and let users define things?
i'm going to argue that both are easy to pull off but the former lacks flexibility while the latter lacks usability. the middle ground is making a solid framework but with shadian extensions.

first let's think what a barebone framework is.

qwex run >
- choose the correct interface
- call submit method > package the source > create run id > create a command that will submit the correct tmpl
- subscribe to the status event (will discuss later)
- subscribe to the log event
- ctrl-c allows you to detach or cancel > call cancel method. detach just exits the cli.
- on finish > prompt: retrieve the artifacts? > download the artifacts into $QWEX_HOME/cache/files/file-id/metadata.json,file-name.ext and symlink to it at $CWD/.qwex/runs/<run-id>/files/file-name.ext

this is the client part and create a command that will submit the correct tmpl is heavy lifting.

so the input of this template compiler will contains:

1. source [ uses: <base plugin>, with: path], storages: [ uses: <base plugin>, with: path ]
2. init: image, command, args (as if it's a github workflow) 
3. run: {{ command }} 
4. exit: eh wait.... OHHHH

i think we could technically make this exactly like github workflow? but maybe simplify or extend it.

so what i like about https://github.com/nektos/act is that you can run github workflow locally. what it lacks is that it doesn't bridge the gap between local and runner env. maybe i wanna start from there? a workflow language with boundary-crossing. crazy lol.

boundary-crossing, compilable workflow language. interesting... very interesting honestly.
and it also allows you to write action in python instead of javascript... or actually whatever you want. (as opposed to github action) 

so it's like act + github action with boundary crossing? lit. lit. honestly fire. but the hardest part is making sure 1. runner has this workflow. 2. runner has the dependency to understand this workflow. 3. make this as light weight as possible 

i think i might accidentally added a new idea

---

github workflow transpiler?

tempting. lol. 

it doesn't have to be jinja2 template. i wanna do something that offers similar idea to github workflow?

the problem is that github workflow has too much feature (like in/output passing) which eventually would require a runtime.  i think we should own our template. making it similar to github workflow, but not too similar

so the problem of describing this as workflow is that it knows how to exit early with fault tolerance. 

chicken and egg of taskfile. the source needs to be transferred in order for the runner to see the taskfile. we can technically string serialize everything sure but we run into another problem of understanding this taskfile. so we need to package the taskfile parser/runner... too? no we're not gonna do that. it will run without knowing the taskfile. sure it might run scripts included in the action but that must be post-source. which is fine, sometimes.

some runner doesn't even know how to containerize things and we have to tell it how.

that's why github workflow fails. actually i don't think there is a tool for this yet. the best solution is that makes the backend understand this job.yaml. LIKE K8S!!!

so im making k8s-less dependency-free task-runner compiler? interesting. 

after thinking and thinking you can see how it boils down to "oh it's just xyz"

---

the problem is that if it's too generic then now compiling to job.yaml becomes very difficult lmao. ok where do i put this at? kinda thing.

as you can see pre-flight and remote have no description.

also this boils down to justfile that you can do so what's the point lmao

run command: 
  git push
  ssh host "cd ./workspaces/repo && git pull && {{ command }} | tee output.log"
  scp  host:./workspaces/repo/output.log ./

or 

run wrapper command:
    wrapped_command=get_wrapper(wrapper) command
    git push
    exec wrapped_command
    scp  host:./workspaces/repo/output.log ./

then oh sharing? i'll post this on my github gist.
   
is that actually the extent of my project

---

now it becomes easier because we can cross boundary only once. the target machine doesn't know anything about qwex. then we have only one wrapper. slurm strategy might be able to wrap singularity strategy tho.

ah fuck it really is taskfile registry. nothing more than that. ðŸ˜­

it sucks what am i doing lmao.

https://taskfile.dev/docs/reference/schema look at the power of taskfile lmao. 

it's really just taskfile presets with boundary fuck it. 

i could make workflow that repeatedly wraps command from above output then submit it. this is the most stupid thing ever lol.

can taskfile technically be compiled into pure bash. idk.  i could contribute the compiler part.

actually where is my goal of contribution now.

---

qwex is a job.yaml that runs taskfile.yaml cross-compiler. it makes job.yaml & task.yaml runs anywhere without k8s or taskfile. but because calling it job.yam or taskfile.yaml makes this looks like it has more features than it has so we will not be calling it kubernetes jobs nor taskfile. instead it just also has compile target of job.yaml. 

the idea of job.yaml is that it requires kubernetes to read it. the idea of task.yaml is that it requires the task binary to read it. now you just define tasks to run and it will be compiled to pure bash to target any runner or container backend. oh you need python? as long as it is std python and your runner is python friendly it will inline that python script for you. 

qwex run

- runs local tasks -> basically just simulate the flight lol
- runs preflight tasks -> git push, packages below takeoff, landing, remote, return into the job description
- runs takeoff tasks -> job submit
- runs landing tasks -> find a landing spot, git pull, mount csi as needed.
- runs taking your luggages -> idk what this is called
- runs remote tasks -> run the actual task (slurm + any containerization)
- runs return tasks -> actually get the artifact back

this gets compiled into

for ssh:
git push
ssh host << EOF
   {{ landing tasks }} 
   sbatch "{{init_headers}} {{ init_command }}"
   sbatch "{{run_headers}}  qwex run {{ command }}" <- can use qwex run now because it assumes init_command installs qwex for it.
EOF

for kubectl:
code --wait /tmp/job.yaml # close to confirm
kubectl apply -y  /tmp/job.yaml

for docker-separated:
{{ skip git push }}
docker compose run --rm
init container 
run container

for docker-simple
{{ skip git push }}
docker compose run --rm
run container with both init command and run command

and of course with caching utility! (qwex miles club: uv cache, container build cache, etc.)

i think we should read this: https://www.reddit.com/r/kubernetes/comments/16tme4n/dockerfile_equivalent_in_kubernetes_cri/

## Week 11: Dec 10, 2025

About SkyPilot...

After reviewing SkyPilot's architecture and features, it's THE EXACT same idea as qwex with the same problem, and the same target audience. 

Good news is that we're not alone and now we don't have to convince anyone that this is a real problem. Bad news is that we have to compete with an established open-source project with a lot of traction. (9k stars!!! on GitHub, wow)

i think skypilot is very heavy. it's a runtime. it's like traveling with a tour guide and see how many people actually like traveling with a tour guide... it's restrictive. travel with a plan. qwex is an itinerary writer. it's like a travel blog. you can follow this to enjoy the most of your trip but you can rewrite parts of it freely.

tour guide can be very effective because it has runtime overhead but it saves cold start. tour guide is very professional so you'd not have jet lag on landing.

is my analogy correct?

## Week 11: Dec 12, 2025

Spec of qwex.

`qwex init` > creates qwex.yaml at root.
1. check if .gitignore exists. if not create one with: .qwex/
2. if .gitignore exists, check if it contains `/\.qwex/?/` then append it.

This is the default config:

```yaml
name: <generated from folder name>

tasks:
  run:
    args:
      command: ""
    steps:
      - name: Echo command
        uses: std/echo
        with:
          message: "Running command: {{ args.command }}"
      - name: Run command as is
        uses: std/base
        with:
          command: "{{ args.command }}"
      - name: Show that run_id has first_class support
        uses: std/echo
        with:
          message: "Run ID: {{ run_id }}"
```

What `qwex run` does is:
1. Load ./qwex.yaml
2. Load .qwex/.env.yaml (if exists) and override the config.
3. Generate a lexicographically sortable unique run_id (e.g., 20231212_153045_abcd1234)
4. Find the task `run` in the config.
5. For each step in task `run`: 
   1. Load the step's `uses` plugin (e.g., std/echo)
   2. Recursively resolve `with` parameters using Jinja2 templating with context:
      - `args`: command-line args passed to `qwex run`
      - `run_id`: generated run ID
   3. Compile the step into a shell command using qwex's core
   4. Append the compiled command to a master shell script along with qwex core. 
6. Finally we should have a shell script that looks like:

```bash
#!/bin/bash

# ... some inlined qwex core functions ...s

# included library instance
std__echo__message() {
  local message="$1"
  echo "$message"
}

__entrypoint__() {
  local command="$1"
  functionforrunningstep "inlined name from step 1, fallback to step 1" "exec std__echo__message 'Running command: $command'"
  functionforrunningstep "inlined name from step 2, fallback to step 2" "exec $command"
  functionforrunningstep "inlined name from step 3, fallback to step 3" "exec std__echo__message 'Run ID: $run_id'"
}
__entrypoint__ "$@"
```

or whatever it compiles to.

## Week 11: Dec 13, 2025

so this is a typical run file:

```yaml
# qwex.yaml
vars:
  # Dependency Injection (Polymorphism)
  # The 'impl' keyword tells the compiler to look for a matching preset
  code:  "{{ impl.code }}"
  agent: "{{ impl.agent }}"

presets:
  simple:
    vars:
      code: modules.github
      agent: modules.ssh_node

modules:
  trace:
    source: std/trace
  
  github:
    source: std/git
    vars: { repo: "git@..." }
  
  ssh_node:
    source: std/ssh
    vars: { host: "cluster-01" }

tasks:
  run_experiment:
    args:
      - command
    
    steps:
      # Phase 1: Local Prep
      - uses: code.push
      
      # Phase 2: The Actor Boundary
      - uses: agent.exec
        with:
          # The Compiler Magic:
          # Qwex compiles this list of steps into a standalone script
          # and passes it to the agent to run.
          steps:
            - uses: code.pull
              with: { mode: "worktree" }
            
            - uses: trace.capture
              with:
                # The 'Inner Run' starts executing this command
                command: "{{ args.command }}"
                # Trace module handles the "Split Horizon" logging here
```

here agent is a concept in qwex where we use jit compilation to compile the qwex core and closure serialize the steps we need to run.

BUTTT the agent itself should be defined in a serializable way itself....

look how i made it:

i wanna do things like elixir but in shell... is that difficult?

how would someone define an agent?

it'd be something like

```
name: ssh
type: agent
vars:
  ...
tasks:
   exec:
        args:
             - task
             - with
        steps:
             - run: |
                     ssh --flags <<EOF
                             {{ includes.core }} # or qwex.core idk
                             # how would we serialize the all tasks here??
                             {{ core.agent.exec }} {{ task }} {{ with }} ????
                      EOF
```
 
this'd be so difficult to pull off LMAO. 

## Week 12: Dec 15, 2025

I started from scratch.

hehe.

you would think i am crazy but i just want to make it simple.
i just want a command wrapper that runs commands remotely.

i think i started from the wrong abstraction. i made jsx before react. i made k8s operator before docker. i made task grammar before the logic of wrapping commands itself.
but i need to get this done tonight or my professor will kill me.
so here's the plan:

i will make reactnode of bash before thinking about making it run "workflows" i will think about rendering it and composing it first.

how would you submit a kubernetes job?

## Week 13: Dec 17, 2025

### Qwex Compilation Pipeline & Module System (Final Design)

**Context:** We've settled on a clean YAML â†’ Bash compilation pipeline with a module system for code reuse.

#### 1. Syntax & Grammar

**Root module (user-written YAML):**
```yaml
name: hello-world

modules:
  log:
    source: qstd/log.yaml
  steps:
    source: qstd/steps.yaml
  module:  # @module is auto-loaded for dependency registration
    source: qstd/module.yaml

vars:
  compile_time_var: "This is inlined at compile time"

tasks:
  greet:
    run: echo "Hello, World!"
  
  say:
    args:
      - name: message
        default: "Hello!"
      - name: times
        default: 1
    vars:
      scoped_var: "Function-scoped variable"
    run: |
      echo "{{ vars.scoped_var }}"
      for i in $(seq 1 {{ args.times }}); do
        echo "{{ args.message }}"
      done
  
  debug:
    run: {{ log.tasks.debug }} "Debugging message"
  
  composite:
    uses: steps.tasks.step  # Inline expansion (macro)
    with:
      - name: "Step 1"
        run: "{{ tasks.greet }}"
      - name: "Step 2"
        run: "{{ tasks.debug }}"
```

**Reference formats (full paths, syntactic sugar deferred):**
- `{{ vars.X }}` - module-level var
- `{{ tasks.greet }}` - task in this module â†’ renders to `hello-world:greet`
- `{{ args.message }}` - task-level arg â†’ renders to `${1:-default}` or `${MESSAGE:-default}`
- `{{ log.tasks.debug }}` - imported module task â†’ renders to `log:debug`
- `{{ log.vars.LEVEL }}` - imported module var
- Shorthand (future): `{{ debug }}`, `{{ log.debug }}` (with precedence lookup)

**Reserved keywords (cannot be module names):**
- `vars`, `tasks`, `args`, `name`, `modules`

#### 2. Compilation Pipeline Overview

**Stage 1: Parse** 
- Load YAML â†’ AST (Module, Task, Args)
- Validate structure

**Stage 2: Resolve**
- Load all modules recursively (e.g., `qstd/log.yaml`, `qstd/steps.yaml`)
- Build flat module registry: `{"__main__": Module, "log": Module, "steps": Module, ...}`
- Construct environment tree (Jinja context)

**Stage 3: Compile to IR**
- For each task, flatten and merge vars/args/tasks into accessible scope
- Render Jinja templates with task-specific context
- Detect function references (`{{ log.tasks.debug }}` â†’ `log:debug`) â†’ dependencies
- Inline `uses/with` blocks (macro expansion)
- Output: `BashScript` IR with functions, dependencies, metadata

**Stage 4: Render to Bash**
- Emit preamble (`#!/bin/bash`, `set -u`)
- Emit `@module` functions (dependency registration, includes)
- Emit all compiled functions with dependency registration lines
- Emit entrypoint (`help` or task dispatch)

#### 3. Environment Tree & Variable Scoping

**Global environment tree (built after module resolution):**
```python
{
  "name": "hello-world",
  "vars": {
    "compile_time_var": "value"
  },
  "tasks": {
    "greet": {
      "run": "echo 'Hello, World!'",
      "vars": {},
      "args": []
    },
    "say": {
      "run": "...",
      "vars": { "scoped_var": "Function-scoped variable" },
      "args": [
        { "name": "message", "default": "Hello!", "positional": 1 },
        { "name": "times", "default": 1, "positional": 2 }
      ]
    }
  },
  "log": {
    "name": "log",
    "vars": { "LEVEL": "DEBUG" },
    "tasks": {
      "debug": { "run": "...", "vars": {}, "args": [...] }
    }
  },
  "steps": {
    "name": "steps",
    "vars": {},
    "tasks": {
      "step": { "run": "...", "vars": {}, "args": [...] },
      "steps": { "run": "...", "vars": {}, "args": [...] }  # Multi-step composite
    }
  },
  "module": {
    "name": "module",
    "tasks": {
      "register_dependency": {...},
      "collect_dependencies": {...},
      "include": {...}
    }
  }
}
```

**Task-scoped Jinja context (when rendering a single task):**
```python
{
  "name": "hello-world",  # root module name
  "vars": { "compile_time_var": "..." },  # root module vars
  "tasks": { "greet": "hello-world:greet", ... },  # task refs (canonical names)
  "args": { "message": "${1:-Hello!}", "times": "${2:-1}" },  # this task's args
  
  # Imported modules (same structure)
  "log": { "name": "log", "vars": {...}, "tasks": {...} },
  "steps": { "name": "steps", "vars": {...}, "tasks": {...} },
  "module": { ... }
}
```

**Scoping rules:**
- Args only visible within task scope
- Task-level vars override root vars (shadow)
- Module context is flat and accessible at any depth
- Jinja sees **flattened** tree; no nesting deeper than `module.vars|tasks|args`

#### 4. Function Visibility & Dependencies

**What each compiled function sees:**
- Access to all root-level and imported module tasks/vars via Jinja context
- Bash cannot access Jinja vars; they're rendered at compile time
- Function dependencies tracked via reference scanning (e.g., `log:debug` in function body)

**Dependency detection:**
- Walk Jinja AST (or simple string scan) for `{{ module.tasks.X }}`
- Map to canonical name: `log:debug`
- Store in `BashFunction.dependencies`
- Render `module:register_dependency "func_name" "dep1 dep2 ..."`

**Execution model (at runtime in bash):**
```bash
module:register_dependency "hello-world:say" "log:debug steps:step"
module:include "hello-world:say"  # Declares all transitive deps first
```

#### 5. Jinja Macros & Inlining (`uses/with`)

**Inlining via Jinja macro (not yet implemented but planned):**
```yaml
debug-vars:
  uses: steps.tasks.step
  with:
    - name: "Step A"
      run: "{{ tasks.greet }}"
    - name: "Step B"
      run: "{{ tasks.debug }}"
```

**Compiler behavior:**
1. Resolve `uses` to task definition (e.g., `steps.step` â†’ load from `qstd/steps.yaml`)
2. Inline the referenced task body
3. Substitute `with` values as positional args or loop vars
4. No new function created; body is copied into caller
5. No dependency on `steps:step` at runtime (already inlined)

**Equivalent to writing:**
```yaml
debug-vars:
  run: |
    {% for step in with %}
      # Inline call to steps.step logic with `step` as context
    {% endfor %}
```

#### 6. Shorthand & Syntactic Sugar (TODO)

**Deferred to Phase 2 (post-MVP):**
- `{{ debug }}` â†’ lookup `vars.debug` or `tasks.debug` with precedence
- `{{ log.debug }}` â†’ lookup `log.vars.debug` or `log.tasks.debug`
- `$message` syntax for args (using Jinja custom filters)

**Explicit namespace required for MVP:**
- Always use `{{ vars.X }}`, `{{ tasks.X }}`, `{{ args.X }}`, `{{ module.tasks.X }}`

#### 7. Module Registry & Include Mechanics

**Module loading (in Resolver):**
1. Parse root YAML
2. For each entry in `modules:`, load `source:` file recursively
3. Build flat registry (no nesting)
4. Validate no reserved keywords used as module names
5. Inject `module` (or `@module`) functions for dependency tracking

**At runtime (bash):**
```bash
# In preamble / header:
module:register_dependency () { ... }
module:collect_dependencies () { ... }
module:include () { ... }

# Before executing user task:
module:include "log:debug"
# This declares log:debug and all its transitive dependencies
```

**Special case: `@module` (or `module`)**
- Always loaded from `qstd/module.yaml` (or provided)
- Contains core functions: `register_dependency`, `collect_dependencies`, `include`
- Cannot be overridden by user

#### 8. Standard Library Modules

**Structure:**
```
qstd/
  module.yaml       # Core: register_dependency, collect_dependencies, include
  log.yaml          # Logging: debug (depends on: utils:once, utils:color)
  utils.yaml        # Utilities: once, color
  steps.yaml        # Steps: step (single), steps (multi)
```

**Dependency graph:**
```
log:debug â†’ utils:once, utils:color
steps:step â†’ (no deps)
steps:steps â†’ (no deps) # Composite task, inlined
module:include â†’ (no deps, builtin)
```

**Example `qstd/log.yaml`:**
```yaml
name: log

tasks:
  debug:
    args:
      - name: message
        default: ""
        positional: 1
    run: |
      if [ "${DEBUG:-0}" -eq 0 ]; then
        return
      fi
      {{ utils.tasks.once }} "utils:color"
      echo -e "$1" >&2
```

**Example `qstd/utils.yaml`:**
```yaml
name: utils

tasks:
  once:
    args:
      - name: key
        positional: 1
      - name: command
        positional: 2
    run: |
      declare -gA UTILS_ONCE_HASHSET
      if [[ -z "${UTILS_ONCE_HASHSET[$1]+x}" ]]; then
        UTILS_ONCE_HASHSET[$1]=1
        eval "$2"
      fi
  
  color:
    run: |
      if [ -t 1 ]; then
        Q_RED='\033[0;31m'
        Q_GREEN='\033[0;32m'
        Q_BLUE='\033[0;34m'
        Q_RESET='\033[0m'
      else
        Q_RED='' Q_GREEN='' Q_BLUE='' Q_RESET=''
      fi
```

#### Implementation Roadmap

**Phase 1 (Current): Full-path references + Module loading**
- [x] Parser (AST)
- [ ] Resolver (module loading, env tree construction)
- [ ] Compiler IR (BashScript, BashFunction with deps)
- [ ] Renderer (bash emission)
- [ ] Test playgrounds (module_run_usage, task_with_args, module_inline, module-uses-with)

**Phase 2 (Future): Syntactic sugar**
- [ ] Shorthand lookup ({{ debug }} â†’ vars/tasks precedence)
- [ ] `$arg` syntax for args
- [ ] Optional args (getopt) & named args

**Phase 3 (Future): Advanced**
- [ ] Automatic dependency detection via Jinja AST walk
- [ ] Inlining macros (`uses/with` â†’ compile-time expansion)
- [ ] Validation & type hints for args/vars

## Week 13: Dec 17, 2025

### Algorithm

1. what if we flattened it? looks better right?

after the first round of semantic analysis we should have

{
  ".": "sourcehash",
  "": "..."
}
and
{
  "sourcehash": {
  vars: {
    kv pairs
  },
  modules: { 
    kv pairs: ModuleSpec(source, vars (overrides))
  },
  tasks: { kv pairs: TaskSpec(... ) }
}}

2. start from root, traverse its tasks
if it's not an inlined task -> do:
- check its dependencies, ([whatever not vars/args].taskname)
- if it's valid (exists in hashtoast[sourcetohash["alias"]]) then continue else raise error here
- build task dependency graph. (a dict of array?) -> if there is a cyclic dependency, raise error here
- if it's an inlined task add it to the inline inline stack.

3. we start from the top of the inline stack, then
- for each pop task in the stack 
- if this task is not an inlined task, skip
- if this task is has more than one inlined tasks, tries to parse every source task
- if at least one of them is an inlined task, add the task itself back to the stack, then add all the to be inlined tasks to the stack
- if can parse successfully (its source task is not an inlined task itself) 

4. then we recursively parse everything in the module source: after the first round of semantic analysis we got the source map: [parent.parent2.alias]-> source hash
then hash->actual source ast. 

not sure if we should prefix the keys with . or suffix with . or none. the index could be "." or simply empty string. this will be used as a prefix when expanding env later so i like "" so that it can be used as-is but index as empty string just sounds weird, because other requires "." you will see later
i think flat namespace is better but could you check if it's implemented the same way i explained?

then continue building task dependency graph here. not sure if we should resolve inline task first then build task dependency but i thought that while we're already looping over non-inline tasks, why not also collect its dependencies

5. we then makes an env map. which is a vars-resolved map of module. start from root again

we first resolve vars like this:  [*module_defined_vars, *import overrides]
we then calculate envhash: hash{sourcehash+varhash}
alias->hash{
  "parent.parent2.alias": "envhash"
},
envhash->canonical module alias {
  "hash": "whatever gets resolved first as traversed bredth-first from root" <- remember to traverse breadth first! use python built-in from collections import deque
}, (for example if std is imported at vars before std imported under utils and no vars override is used, then hashes are the same, meaning canonical is std instead of utils.std and utils.std will simply be resolved to std )
{
  [envhash]: {
    vars: resolved vars
  }
}

then after canonical module alias is resolved, we can start a new loop from the root and traverse through task dependency graph root

first we calculate taskenvhash

not sure if this is accurate?

hash(envhash, hash(TaskSpec.vars), hash(taskname)) -> taskhash

{
  taskhash: tasknode
}

if not exist we compile tasknode

TaskNode(
  name="get its module env hash -> get its module canonical name -> become canonalias.taskname"
  run="<vars.varname>" -> render the variable first -> it might become <alias.taskname> later then we render the
  <alias.taskname> by just simply looking up the envhash then choose canonical taskname by canonical.module.alias.taskname"
  deps: [canonical taskname]
)

then we add task node to the stack

we then have a stack of tasknode

we simply render that

wdyt?

---

## Dec 17, 2025 (Evening): Improved Compilation Algorithm

**Note:** Ignore the previous todo items listed earlier in this logbook. This section describes a significantly improved algorithm for Phase 2.

### Design Goals

The current Phase 1 implementation (working, all tests passing) has several inefficiencies:
- Loads all modules eagerly, including unused ones
- Builds a nested env tree, then flattens it during rendering
- Separates canonicalization from inline expansion (two passes)
- Uses regex post-render to detect dependencies (lossy and fragile)
- No deduplication of identical task bodies

### Improved Algorithm (Phase 2)

This algorithm addresses all of the above issues with a single-pass, lazy, hash-indexed approach.

#### Key Principles

1. **Flat namespace**: No separation between `alias.tasks.X` and `alias.vars.Y`. Use `alias.X` for both tasks and vars. Enforce name-clash prevention during module parsing.
2. **Lazy module loading**: Only load modules when they're referenced by reachable tasks.
3. **Immediate canonicalization**: Resolve canonical module aliases during dep traversal, not in a separate phase.
4. **Inline var context**: Inlined tasks use vars from their **source module** (where defined), not the caller's module.
5. **Body-hash deduplication**: Emit each unique rendered body only once, regardless of how many aliases point to it.
6. **Auto-inject `@module`**: Built-in module functions (`register_dependency`, `collect_dependencies`, `include`) are always available without explicit import.

#### Data Structures

```python
# Flat maps (no nested trees)
alias_to_source_hash: Dict[str, str]          # "." or "utils" -> source_hash
source_hash_to_ast: Dict[str, ModuleAST]      # source_hash -> parsed AST

# Env resolution (hash-indexed for deduplication)
alias_to_env_hash: Dict[str, str]             # alias -> env_hash
env_hash_to_canonical_alias: Dict[str, str]   # env_hash -> first alias (BFS)
env_hash_to_env: Dict[str, EnvVars]           # env_hash -> resolved vars

# Task dependencies (canonical names only)
dep_graph: Dict[str, Set[str]]                # canon_fqn -> set(canon_deps)

# Deduplication
body_hash_to_canonical_fqn: Dict[str, str]    # body_hash -> first canon_fqn
task_nodes: List[TaskNode]                    # unique tasks to emit
```

#### Algorithm Phases

**Phase 1: Lazy Load + Dep Traversal + Canonicalization (Single Pass)**

```python
from collections import deque

visited = set()  # (alias, name) to detect cycles
inline_stack = []  # tasks needing inline expansion
dep_graph = {}

# Start from root module's tasks
root_tasks = [f".:{t}" for t in root_ast.tasks.keys()]
q = deque(root_tasks)

while q or inline_stack:
    if q:
        fqn = q.popleft()  # e.g., "utils:color"
        alias, name = fqn.split(":")
        
        # Cycle detection
        if (alias, name) in visited:
            continue
        visited.add((alias, name))
        
        # Lazy load module if not already loaded
        if alias not in alias_to_source_hash:
            load_module(alias)  # Parse YAML, compute source_hash, cache
        
        # Lazy resolve env if not already resolved
        if alias not in alias_to_env_hash:
            source_hash = alias_to_source_hash[alias]
            ast = source_hash_to_ast[source_hash]
            resolved_vars = merge(ast.vars, parent_overrides)
            env_hash = hash(source_hash, hash_dict(resolved_vars))
            alias_to_env_hash[alias] = env_hash
            
            # Canonical alias: first-seen wins (BFS guarantees root priority)
            if env_hash not in env_hash_to_canonical_alias:
                env_hash_to_canonical_alias[env_hash] = alias
                env_hash_to_env[env_hash] = {"vars": resolved_vars}
        
        # Get task from AST
        ast = source_hash_to_ast[alias_to_source_hash[alias]]
        item = ast.items[name]  # Could be task or var
        
        if not is_task(item):
            continue  # Skip vars during dep traversal
        
        # Handle inline tasks
        if item.is_inlined():
            inline_stack.append((alias, name, item))
            # Enqueue inline sources for dep traversal
            for src_ref in item.inlined_sources():
                src_alias, src_name = parse_ref(src_ref)
                q.append(f"{src_alias}:{src_name}")
            continue
        
        # Collect deps and canonicalize immediately
        deps = collect_deps(item)  # e.g., ["utils.color", "log.debug"]
        canonical_deps = set()
        for dep_ref in deps:
            dep_alias, dep_name = parse_ref(dep_ref)
            
            # Lazy load dep module
            if dep_alias not in alias_to_env_hash:
                load_and_resolve_env(dep_alias)
            
            # Canonicalize dep reference
            dep_env_hash = alias_to_env_hash[dep_alias]
            dep_canon_alias = env_hash_to_canonical_alias[dep_env_hash]
            canonical_dep = f"{dep_canon_alias}:{dep_name}"
            canonical_deps.add(canonical_dep)
            q.append(canonical_dep)  # Traverse transitive deps
        
        # Canonicalize this task's name
        env_hash = alias_to_env_hash[alias]
        canon_alias = env_hash_to_canonical_alias[env_hash]
        canon_fqn = f"{canon_alias}:{name}"
        dep_graph[canon_fqn] = canonical_deps
    
    # Process inline expansion stack (after deps)
    if not q and inline_stack:
        alias, name, item = inline_stack.pop()
        
        # Check if all inline sources are resolved (non-inline)
        all_resolved = True
        for src_ref in item.inlined_sources():
            src_alias, src_name = parse_ref(src_ref)
            src_ast = source_hash_to_ast[alias_to_source_hash[src_alias]]
            if src_ast.items[src_name].is_inlined():
                all_resolved = False
                inline_stack.append((alias, name, item))  # Retry later
                break
        
        if not all_resolved:
            continue
        
        # Expand inline: use **source module's vars**, not caller's
        expanded = inline_expand(
            item,
            source_module_vars=env_hash_to_env[alias_to_env_hash[src_alias]]["vars"]
        )
        
        # Replace in AST and re-enqueue
        ast = source_hash_to_ast[alias_to_source_hash[alias]]
        ast.items[name] = expanded
        q.append(f"{alias}:{name}")

# Cycle check
assert acyclic(dep_graph)
```

**Phase 2: Task Deduplication by Body Hash**

```python
body_hash_to_canonical_fqn = {}
task_nodes = []

for canon_fqn in dep_graph.keys():
    alias, name = canon_fqn.split(":")
    env_hash = alias_to_env_hash[alias]
    env = env_hash_to_env[env_hash]
    ast = source_hash_to_ast[alias_to_source_hash[alias]]
    task = ast.items[name]
    
    # Render body (deps already canonicalized)
    rendered_body = render(task.run, vars=env["vars"], args=task.args)
    body_hash = hash(rendered_body)
    
    # Deduplicate by body hash
    if body_hash in body_hash_to_canonical_fqn:
        continue  # Skip duplicate; first canonical wins
    
    body_hash_to_canonical_fqn[body_hash] = canon_fqn
    node = TaskNode(
        name=canon_fqn,
        body=rendered_body,
        deps=list(dep_graph[canon_fqn])
    )
    task_nodes.append(node)
```

**Phase 3: Emission**

```python
# Preamble (shebang, set -u)
# @module internals (auto-injected)
# task_nodes (function defs + dependency registration)
# help (lists root tasks only)
# entrypoint
render_bash(task_nodes, root_tasks)
```

### Comparison with Current Implementation

| Aspect | Current (Phase 1) | Improved (Phase 2) |
|--------|-------------------|-------------------|
| Module loading | Eager (all modules) | Lazy (only reachable) |
| Env structure | Nested tree | Flat hash-indexed map |
| Canonicalization | Separate loop after resolution | Immediate during dep traversal |
| Inline expansion | Separate phase | Integrated with dep traversal |
| Dep detection | Regex post-render | AST-based pre-render |
| Task deduplication | None (emit all) | Body-hash deduplication |
| Namespace | Separate `tasks.X` and `vars.Y` | Unified `alias.X` |
| Unused tasks | Emitted anyway | Never loaded or emitted |

### What Needs to Change

1. **Resolver**: Replace `_build_env_tree()` with flat hash-indexed maps. Remove nested dict construction.
2. **Compiler**: Replace `_compile_task()` loop with BFS dep traversal. Integrate inline expansion into the traversal.
3. **Dep detection**: Replace regex `_detect_dependencies()` with AST walk before Jinja render.
4. **AST spec**: Merge tasks and vars into a single `items` dict. Add name-clash validation in `Module.from_dict()`.
5. **Deduplication**: Add body-hash check before emitting TaskNode.
6. **Auto-inject**: Add `@module` to every compilation without user import.

### Benefits

- **Performance**: Only loads/compiles reachable tasks. Typical 50-80% reduction in compile time for large projects.
- **Correctness**: AST-based dep detection is precise; no false positives/negatives from regex.
- **Size**: Deduplication reduces output script size by 30-60% when modules are reused.
- **Maintainability**: Single-pass algorithm is easier to reason about and debug.

---

## Dec 17, 2025: Args vs Vars Namespace Debate

### The Question

Should we keep `args` as a separate field in `Task`, or merge it into `vars`?

**Current Design (Phase 1):**
```python
@dataclass
class Arg:
    name: str
    default: Any = None
    positional: int = 0

@dataclass
class Task:
    args: List[Arg]  # Separate namespace
    vars: Dict[str, Any]
```

**Proposed Design (Phase 2):**
```python
@dataclass
class Task:
    vars: Dict[str, Any]  # Unified namespace, includes {{ $1 }} refs
    # args field removed
```

### Initial Position: Keep Separate

**Arguments FOR separation:**
1. **Explicit positional semantics**: `Arg(positional=1)` makes runtime params obvious
2. **Inline expansion clarity**: Parent knows which fields are compile-time (vars) vs runtime (args)
3. **Collision prevention**: Prevents parent from accidentally overriding runtime params with `with:`

### The Rebuttal

**User's counterarguments:**
1. **`$1` IS explicit**: Template syntax `{{ $1 }}` is MORE explicit than metadata field `Arg(positional=1)`
2. **Current design breaks inline expansion**: Args are inaccessible during `uses/with` expansion
3. **Dict `with:` should override everything**: No reason to hide args from parent; let parent customize all vars

**Example of the problem:**
```yaml
# Current (Phase 1) - BROKEN
tasks:
  deploy:
    args:
      - {name: environment, positional: 1}
    vars:
      flags: "--verbose"
    run: |
      echo "Deploying to {{ environment }}"
      deploy.sh {{ flags }}

  deploy_prod:
    uses: deploy
    with:
      flags: "--quiet"  # âœ“ Can override vars
      # âŒ CANNOT override args.environment!
```

With merged namespace:
```yaml
# Proposed (Phase 2) - FIXED
tasks:
  deploy:
    vars:
      environment: "{{ $1 }}"  # Explicit bash param
      flags: "--verbose"
    run: |
      echo "Deploying to {{ environment }}"
      deploy.sh {{ flags }}

  deploy_prod:
    uses: deploy
    with:
      environment: "production"  # âœ“ Can override!
      flags: "--quiet"
```

### Resolution: Merge Into Vars

**Why the merge is better:**

1. **Template-as-metadata**: `{{ $1 }}` in the template IS the positional metadata. We don't need a separate dataclass to track what's already explicit in the code.

2. **Inline expansion works**: Parent can override both compile-time defaults and runtime params via `with:` dict. Current design makes args invisible to parent, which breaks customization.

3. **Consistent override semantics**: Everything in `vars` can be overridden by parent. No special cases, no hidden fields.

4. **Simpler AST**: Remove `Arg` dataclass entirely. Parser just needs to detect `{{ $N }}` pattern for validation/analysis.

**What changes:**
- Remove `Arg` dataclass from `ast/spec.py`
- Update `Task.from_dict()` to parse args-like vars (detect `{{ $N }}` for positional inference)
- Update inline expansion: `with:` dict can override any var, including ones with `{{ $1 }}` refs
- Emit behavior unchanged: `{{ $1 }}` still renders as bash positional param

**Benefits:**
- Unified namespace: all task inputs in one place
- Flexible parent overrides: no arbitrary restrictions
- Cleaner AST: one less dataclass to maintain
- More Pythonic: "explicit is better than implicit" (the template IS explicit)

---

## Dec 18, 2025: Compiler Refactor & Playground Standardization

### Overview

Based on the feedback from Dec 17, we have refactored the compiler and standard library to simplify the namespace model and standardize the playground.

### Changes Implemented

1.  **Removed `args` Namespace**
    *   Removed `Arg` dataclass from AST.
    *   Removed `args:` field from `Task`.
    *   Adopted native Bash positional parameters (`$1`, `$2`) directly in templates.
    *   For semantic naming, users can alias them in `vars:` (e.g., `message: "$1"`).

2.  **Flattened Environment Model**
    *   Removed `vars.` and `tasks.` prefixes.
    *   Variables and tasks are now accessible directly in the Jinja context (e.g., `{{ color }}` instead of `{{ vars.color }}`, `{{ log.debug }}` instead of `{{ log.tasks.debug }}`).
    *   `Resolver` now produces a flattened dictionary instead of a nested tree.

3.  **Standard Library Updates**
    *   `log.yaml`, `utils.yaml`, `steps.yaml` updated to new spec.
    *   `utils.color` now calls `once` internally.
    *   `steps.step` renamed to `steps.compose`.

4.  **Playground Standardization**
    *   Renamed directories to kebab-case:
        *   `module_inline` â†’ `module-inline`
        *   `module_run_usage` â†’ `module-run`
        *   `task_with_args` â†’ `task-with-args`
    *   Updated all `qwex.yaml` files to reflect the new spec.
    *   Added `playground/qwex.yaml` orchestration workflow to test all examples.

### Migration Guide

| Old Spec | New Spec |
| :--- | :--- |
| `{{ args.name }}` | `$1` (or alias in vars) |
| `{{ vars.color }}` | `{{ color }}` |
| `{{ log.tasks.debug }}` | `{{ log.debug }}` |
| `tasks.step` | `tasks.compose` |

---

## Dec 18, 2025: QWL Compilation Pipeline - Detailed Specification

### Executive Summary

The QWL (Qwex Workflow Language) compiler transforms YAML task definitions into executable Bash scripts using a **3-phase pipeline**: Resolution â†’ Compilation â†’ Rendering. This document details the complete compilation algorithm, data structures, and implementation choices.

### Why Not Use Jinja2 Extensions for `{% qx %}`?

**Question:** Why process `{% qx %}...{% xq %}` blocks manually instead of using Jinja2's extension system?

**Answer:** We **do** use Jinja2, just not for the qx block itself. Here's why:

1. **qx blocks are bash constructs, not Jinja syntax**
   - `{% qx %}` generates a **heredoc** (`<< 'EOF'`), which is valid bash, not a Jinja control structure
   - The content inside needs `$` escaped as `\$` so variables evaluate **on the remote shell**, not locally
   - Jinja2 would try to render `{{ module.task }}` references during template processing, but we need to detect them first to include dependencies

2. **Pre-processing is simpler and more explicit**
   - We extract qx blocks, analyze them for dependencies, and generate the heredoc structure
   - Then Jinja2 renders the rest of the template (including the heredoc content variables)
   - This gives us full control over when and how task references are resolved

3. **Separation of concerns**
   - **qx preprocessing**: Bash-level concern (heredocs, remote execution boundaries)
   - **Jinja rendering**: Template-level concern (variable substitution, task references)
   - Mixing these would require complex custom Jinja2 extensions that understand bash escaping rules

**What Jinja2 IS used for:**
- Rendering `{{ variable }}` references
- Rendering `{{ module.task }}` task references (which become canonical names like `log:debug`)
- Processing `uses/with` blocks for task composition
- Template inheritance (future feature)

**The pipeline is:**
```
YAML â†’ AST â†’ qx preprocessing â†’ Jinja rendering â†’ Bash IR â†’ Bash script
                â†‘                    â†‘
                bash-aware           template-aware
```

### Phase 1: Resolution (Resolver)

**Purpose:** Load modules, resolve imports, build environment maps.

#### Data Structures

The resolver maintains two parallel index systems for backward compatibility:

**Legacy Cache (Phase 1):**
```python
_module_cache: Dict[str, Module]      # alias -> Module AST
_source_map: Dict[str, Path]          # alias -> source file path
```

**Hash-Indexed Maps (Phase 2):**
```python
alias_to_source_hash: Dict[str, str]           # "log" -> "a3f2..."
source_hash_to_ast: Dict[str, Module]          # "a3f2..." -> Module AST
source_hash_to_path: Dict[str, Path]           # "a3f2..." -> Path
alias_to_env_hash: Dict[str, str]              # "log" -> "b8d1..."
env_hash_to_canonical_alias: Dict[str, str]    # "b8d1..." -> "log" (first seen)
env_hash_to_env: Dict[str, Dict[str, Any]]     # "b8d1..." -> {vars, tasks}
```

**Hash Functions:**
```python
source_hash = sha256(file_bytes)[:16]
env_hash = sha256(f"{source_hash}:{json(vars_dict)}")[:16]
```

**Why Two Systems?**
- Source hash: identifies unique file content
- Env hash: identifies unique instantiation (source + vars binding)
- Two modules with same source but different vars â†’ different env_hash
- Two modules with same source + vars â†’ same env_hash â†’ deduplication

#### Resolution Algorithm

```python
def resolve(root_module: Module) -> Dict[str, Any]:
    """
    1. Clear previous state
    2. Register root module with alias "" and synthetic hash
    3. Load all imports recursively (eager, will be lazy in future)
    4. Build flat hash-indexed maps
    5. Return nested env tree (backward compatibility)
    """
```

**Step 1: Module Loading (Recursive)**
```python
def _load_modules_recursive(module, visited=None, module_dir=None):
    for mod_ref in module.modules.values():
        # Resolve source with builtin fallback
        source_path = _resolve_module_source(mod_ref.source, module_dir)
        
        # Parse and cache
        loaded = parser.parse_file(source_path)
        _module_cache[mod_ref.name] = loaded
        
        # Index by hash
        source_hash = sha256(source_path.read_bytes())[:16]
        alias_to_source_hash[mod_ref.name] = source_hash
        source_hash_to_ast[source_hash] = loaded
        
        # Recurse with module's directory as new base
        self._load_modules_recursive(loaded, visited, source_path.parent)
```

**Builtin Resolution:**
```python
def _resolve_module_source(source: str, module_dir: Path) -> Path:
    """
    Resolution order:
    1. Relative to current module's dir: ./local.yaml
    2. Relative with .yaml added: ./local â†’ ./local.yaml
    3. Builtin modules: std/log â†’ lib/qwl/builtins/std/log.yaml
    4. Builtin with .yaml: std/log â†’ lib/qwl/builtins/std/log.yaml
    """
    # Try local paths first
    if (module_dir / f"{source}.yaml").exists():
        return module_dir / f"{source}.yaml"
    
    # Try builtins
    if (_BUILTINS_DIR / f"{source}.yaml").exists():
        return _BUILTINS_DIR / f"{source}.yaml"
    
    raise FileNotFoundError(...)
```

**Step 2: Build Flat Maps**
```python
def _build_flat_maps(module, alias, parent_vars, visited=None):
    # Compute resolved vars (parent + module vars)
    resolved_vars = {**parent_vars, **module.vars}
    
    # Compute env hash
    source_hash = alias_to_source_hash[alias]
    env_hash = _hash_env(source_hash, resolved_vars)
    
    # Register canonical alias (first wins in BFS)
    if env_hash not in env_hash_to_canonical_alias:
        env_hash_to_canonical_alias[env_hash] = alias
    
    # Build and store environment
    env = _build_module_env(module, alias, resolved_vars)
    env_hash_to_env[env_hash] = env
    
    # Recurse to imports
    for mod_name in module.modules:
        self._build_flat_maps(_module_cache[mod_name], mod_name, resolved_vars)
```

**Step 3: Build Environment Dict**
```python
def _build_module_env(module, alias, resolved_vars) -> Dict[str, Any]:
    """
    Returns flattened dict:
    {
        "color": "blue",           # vars at root
        "module_name": "utils",
        "once": "utils:once",      # task mappings
        "color": "utils:color"     # (if imported; root tasks just use name)
    }
    """
    env = dict(resolved_vars)
    env["module_name"] = module.name
    
    for task_name in module.tasks:
        if alias == "":
            env[task_name] = task_name  # Root: "greet"
        else:
            env[task_name] = f"{alias}:{task_name}"  # "log:debug"
    
    return env
```

**Step 4: Build Nested Tree (Backward Compatibility)**
```python
def _build_env_tree(module, is_root=True) -> Dict[str, Any]:
    """
    Returns nested structure for Jinja:
    {
        "color": "blue",
        "greet": "greet",
        "log": {
            "debug": "log:debug",
            "info": "log:info",
            ...
        }
    }
    """
```

### Phase 2: Compilation (Compiler)

**Purpose:** Transform AST into Bash IR using BFS traversal and dependency detection.

#### Algorithm Overview

```
1. Resolve modules (call Resolver)
2. BFS from root tasks to discover reachable dependencies
3. Compile only reachable tasks with canonical aliasing
4. Body-hash deduplication to avoid emitting duplicate functions
5. Generate help function
```

#### BFS Traversal Implementation

```python
def _compile_with_bfs(root_module, env_tree, root_task_names):
    queue = deque()  # [(alias, task_name), ...]
    visited = set()  # {canonical_fqn, ...}
    body_hash_to_fqn = {}  # {body_hash: first_fqn}
    compiled = []  # [(fqn, BashFunction), ...]
    
    # Seed with root tasks
    for task_name in root_task_names:
        queue.append(("", task_name))
    
    while queue:
        alias, task_name = queue.popleft()
        
        # Get canonical FQN
        canonical_fqn = resolver.get_canonical_fqn(alias, task_name)
        if canonical_fqn in visited:
            continue
        visited.add(canonical_fqn)
        
        # Get module and task
        if alias == "":
            mod = root_module
            mod_env = env_tree
        else:
            mod = resolver._module_cache[alias]
            mod_env = env_tree[alias]
        
        task = mod.tasks[task_name]
        
        # Detect dependencies (AST-based, before rendering)
        if task.uses:
            deps = _detect_deps_from_uses(task.uses, task.with_, mod_env)
        else:
            deps = _detect_deps_from_template(task.run, mod_env)
        
        # Enqueue dependencies
        for dep_alias, dep_task in deps:
            queue.append((dep_alias, dep_task))
        
        # Compile task
        fn = _compile_task(alias, task, mod_env)
        
        # Body-hash deduplication
        body_hash = sha256(fn.body.encode())[:16]
        if body_hash not in body_hash_to_fqn:
            body_hash_to_fqn[body_hash] = canonical_fqn
            compiled.append((canonical_fqn, fn))
    
    return [fn for _, fn in compiled]
```

**Key Points:**
- **AST-based dependency detection**: Scan template for `{{ module.task }}` patterns BEFORE rendering
- **Canonical FQN**: `get_canonical_fqn(alias, task)` uses env_hash to deduplicate equivalent modules
- **Body-hash dedup**: If two tasks compile to identical bash code, emit only once

#### Dependency Detection

**From Template (AST-based):**
```python
def _detect_deps_from_template(template: str, env_tree) -> Set[Tuple[str, str]]:
    """
    Regex: {{ module.task }}
    Returns: {(alias, task_name), ...}
    
    Example:
    "{{ log.debug }}" â†’ [("log", "debug")]
    
    Verification:
    - Check env_tree["log"]["debug"] exists
    - Check it's a canonical name (contains ":")
    - Extract (alias, task) from "log:debug"
    """
    pattern = r"\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)"
    deps = set()
    
    for module_ref, task_ref in re.findall(pattern, template):
        if module_ref in env_tree and isinstance(env_tree[module_ref], dict):
            module_env = env_tree[module_ref]
            if task_ref in module_env:
                canonical = module_env[task_ref]
                if ":" in canonical:
                    alias, task_name = canonical.split(":", 1)
                    deps.add((alias, task_name))
    
    return deps
```

**From uses/with:**
```python
def _detect_deps_from_uses(uses, with_items, env_tree) -> Set[Tuple[str, str]]:
    """
    uses: "steps.compose"
    
    1. Parse uses reference: module.task â†’ (module, task)
    2. Look up canonical name in env_tree
    3. Scan with_items for nested run: templates
    4. Return all dependencies
    """
```

#### Task Compilation

```python
def _compile_task(module_name, task, env_tree) -> BashFunction:
    # Generate function name
    fn_name = task.name if module_name == "" else f"{module_name}:{task.name}"
    
    # Build task-local context
    task_context = dict(env_tree)
    if task.vars:
        task_context.update(task.vars)
    
    # Render body
    if task.uses:
        body = _compile_uses_with(task.uses, task.with_, env_tree, task_context)
    else:
        body = _render(task.run, task_context)
    
    # Detect deps from rendered body (for rendered check)
    deps = _detect_dependencies(body)  # Regex: \b(module:task)\b
    
    return BashFunction(name=fn_name, body=body, dependencies=list(deps))
```

### Phase 3: Rendering (Template Processing)

**Purpose:** Convert template strings to executable bash using Jinja2.

#### The Two-Stage Rendering Process

```python
def _render(template: str, context: Dict[str, Any]) -> str:
    """
    1. Process {% qx %}...{% xq %} blocks (bash-level)
    2. Render with Jinja2 (template-level)
    """
    # Stage 1: qx preprocessing
    template = _process_qx_blocks(template, context)
    
    # Stage 2: Jinja rendering
    from jinja2 import Environment
    env = Environment()
    tmpl = env.from_string(template)
    return tmpl.render(**context)
```

#### Stage 1: qx Block Preprocessing

**Purpose:** Generate heredoc constructs for remote execution boundaries.

```python
def _process_qx_blocks(template: str, context) -> str:
    """
    Pattern: {% qx %}...{% xq %}
    
    Replacement:
    << 'QWEX_A3F2B8D1'
    $(module:include log:debug utils:color)
    \$escaped content here
    QWEX_A3F2B8D1
    """
    pattern = r"\{%\s*qx\s*%\}(.*?)\{%\s*xq\s*%\}"
    
    def replace_qx_block(match):
        block_content = match.group(1)
        return _generate_heredoc_block(block_content, context)
    
    return re.sub(pattern, replace_qx_block, template, flags=re.DOTALL)
```

**Heredoc Generation:**
```python
def _generate_heredoc_block(block_content, context) -> str:
    import uuid
    
    # Unique delimiter
    heredoc_id = f"QWEX_{uuid.uuid4().hex[:8].upper()}"
    
    # Detect task refs: {{ module.task }}
    task_refs = _detect_task_refs_in_block(block_content, context)
    
    # Escape $ as \$ for remote evaluation
    escaped_content = block_content.replace("$", "\\$")
    
    # Build heredoc
    if task_refs:
        deps_list = " ".join(sorted(task_refs))
        return f"""<< '{heredoc_id}'
$(module:include {deps_list})
{escaped_content}
{heredoc_id}"""
    else:
        return f"""<< '{heredoc_id}'
{escaped_content}
{heredoc_id}"""
```

**Task Reference Detection in qx Blocks:**
```python
def _detect_task_refs_in_block(block, context) -> Set[str]:
    """
    Find {{ module.task }} patterns and resolve to canonical names.
    Also detect already-canonical module:task patterns.
    
    Returns: {'log:debug', 'utils:color'}
    """
    refs = set()
    
    # Pattern 1: {{ module.task }}
    pattern = r"\{\{\s*([a-zA-Z_][a-zA-Z0-9_]*)\.([a-zA-Z_][a-zA-Z0-9_]*)"
    for module_ref, task_ref in re.findall(pattern, block):
        if module_ref in context:
            module_env = context[module_ref]
            if task_ref in module_env:
                canonical = module_env[task_ref]
                refs.add(canonical)
    
    # Pattern 2: module:task (already canonical)
    canonical_pattern = r"\b([a-zA-Z_][a-zA-Z0-9_]*:[a-zA-Z_][a-zA-Z0-9_]*)\b"
    refs.update(re.findall(canonical_pattern, block))
    
    return refs
```

**Why Escape $?**
```bash
# Without escaping (WRONG):
ssh remote << 'EOF'
echo $USER  # Evaluates locally! Sends empty string if $USER not set
EOF

# With escaping (CORRECT):
ssh remote << 'EOF'
echo \$USER  # Evaluates on remote shell
EOF
```

#### Stage 2: Jinja2 Rendering

**What Jinja2 Renders:**
1. Variable substitution: `{{ color }}` â†’ `"blue"`
2. Task references: `{{ log.debug }}` â†’ `log:debug`
3. Conditional logic: `{% if condition %}`
4. Loops: `{% for item in items %}`

**Example:**
```jinja
# Template
{{ log.debug }} "Starting task"
echo "Color is {{ color }}"

# Context
{
  "log": {"debug": "log:debug"},
  "color": "blue"
}

# Rendered
log:debug "Starting task"
echo "Color is blue"
```

### Complete Example Walkthrough

**Input YAML:**
```yaml
name: example
vars:
  color: blue
modules:
  log:
    source: std/log
tasks:
  greet:
    run: |
      {{ log.debug }} "Hello"
      echo "Color: {{ color }}"
```

**Resolution Phase:**
1. Load root module
2. Load `std/log` from `lib/qwl/builtins/std/log.yaml`
3. Build env_tree:
```python
{
  "color": "blue",
  "greet": "greet",
  "log": {
    "debug": "log:debug",
    "info": "log:info",
    ...
  }
}
```

**Compilation Phase:**
1. BFS: Start from `greet`
2. Detect deps: `{{ log.debug }}` â†’ `("log", "debug")`
3. Enqueue `("log", "debug")`
4. Compile `log:debug` first (dependency)
5. Compile `greet` (root task)
6. Body-hash check: no duplicates
7. Generate help function

**Rendering Phase:**
1. No qx blocks, skip preprocessing
2. Jinja render:
   - `{{ log.debug }}` â†’ `log:debug`
   - `{{ color }}` â†’ `blue`
3. Final bash:
```bash
log:debug "Hello"
echo "Color: blue"
```

**Output BashScript IR:**
```python
BashScript(
  functions=[
    BashFunction(name="log:debug", body="...", dependencies=[]),
    BashFunction(name="greet", body='log:debug "Hello"\necho "Color: blue"', dependencies=["log:debug"]),
    BashFunction(name="help", body="...", dependencies=[])
  ],
  available_tasks=["greet"]
)
```

### Key Design Decisions

1. **Why hash-indexed maps?**
   - Enables deduplication of equivalent module instantiations
   - O(1) lookup for canonical aliases
   - Future: enables lazy loading (only load reachable modules)

2. **Why AST-based dependency detection?**
   - More reliable than regex on rendered output
   - Can detect deps before rendering (important for BFS)
   - Handles nested templates correctly

3. **Why BFS instead of DFS?**
   - Natural ordering: dependencies before dependents
   - First-seen canonical alias makes sense in BFS order
   - Easy to add cycle detection

4. **Why body-hash deduplication?**
   - Two tasks with identical code should only emit one function
   - Reduces bash script size
   - Common with parameterized tasks that resolve to same code

5. **Why two-stage rendering (qx + Jinja)?**
   - qx blocks need bash-aware escaping (`$` â†’ `\$`)
   - Dependency detection needs to happen before Jinja renders
   - Clean separation of concerns: bash-level vs template-level

### Differences from Original Plan

**Original Plan (from earlier discussions):**
- Single-pass compilation
- Nested environment tree only
- No hash-indexed maps
- No builtin module resolution

**Current Implementation:**
- âœ… Two-pass: resolution + compilation
- âœ… Dual indexing: nested tree (compatibility) + flat maps (performance)
- âœ… Hash-based deduplication (source + env)
- âœ… Builtin module auto-resolution
- âœ… qx boundary preprocessing
- âœ… AST-based dependency detection

**Why These Changes?**
- Hash maps enable future lazy loading
- Builtin resolution makes stdlib "just work"
- qx preprocessing enables remote execution patterns
- AST-based deps are more robust than rendered regex

### Future Enhancements

1. **Lazy Module Loading**
   - Currently: load all modules eagerly
   - Future: load only during BFS traversal
   - Benefit: faster startup for large projects

2. **Incremental Compilation**
   - Track source_hash changes
   - Only recompile affected tasks
   - Cache compiled functions

3. **Parallel Compilation**
   - BFS levels can be compiled in parallel
   - Tasks with no interdependencies are independent

4. **Advanced qx Features**
   - Named boundaries: `{% qx "boundary_name" %}`
   - Context passing: `{% qx with vars %}`
   - Nested boundaries (boundary within boundary)